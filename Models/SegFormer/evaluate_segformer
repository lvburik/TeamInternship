# %% imports
import numpy as np
import os
import glob
from PIL import Image
from scipy.ndimage import gaussian_filter
import tensorflow as tf
import matplotlib.pyplot as plt
from transformers import SegformerImageProcessor, Trainer, TrainingArguments
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor
import piq

#torch imports
import torch
from torchvision import transforms
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from torch.optim import AdamW
from torchmetrics.functional import structural_similarity_index_measure as ssim
import torch.nn.functional as F
from torchmetrics import JaccardIndex
from segmentation_models_pytorch.losses import DiceLoss

# data
from Resin_plates import *
from data import *

#Import model

INPUT_SIZE = 512 
batch_size = 16
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

feature_extractor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512", ignore_mismatched_sizes=True, num_labels=2)
model.decode_head.classifier = nn.Conv2d(
    in_channels=256,
    out_channels=2,  
    kernel_size=1
)
weights_path = "segformer_50ep.pth"
state_dict = torch.load(weights_path, map_location="cpu")  

model.load_state_dict(state_dict)
model.eval()

#Import masks and video

path = "Resin_plates/circular.png"
image=Image.open(path)
mask_c=np.array(image)
#print(mask_c.shape)

path_c = "Resin_plates/rec.png"
image_c=Image.open(path_c)
mask_rec=np.array(image_c)
#print(mask_rec.shape)

path_s = "Resin_plates/square.png"
image_s=Image.open(path_s)
mask_s=np.array(image_s)
#print(mask_s.shape)

path_t = "Resin_plates/tri.png"
image_t=Image.open(path_t)
mask_t=np.array(image_t)
#print(mask_t.shape)

print("Masks are loaded")

masks=[mask_c, mask_rec, mask_s, mask_rec]
frames=[]

video=np.load("circular_2_lamps_right_off.npy").reshape(-1, 480, 640)
video_files=[video]

print("video is loaded")

for video in video_files:
    sampling_rate = 50
    sampled_frames = video[::sampling_rate]

# Preprocess

def preprocess(frame, min, max):
    norm_frame = (frame - np.min(frame)) / (max - min)
    norm_frame = np.clip(norm_frame, 0, 1)
    norm_frame=norm_frame.astype(np.float32)
    norm_frame=np.squeeze(norm_frame)
    rgb = plt.cm.jet(norm_frame)[:, :,:3]
    norm_frame_uint8 = (rgb * 255).astype(np.uint8) 
    pil_img = Image.fromarray(norm_frame_uint8)
    return pil_img

def label_tensor(mask):
    label = Image.fromarray(mask.astype(np.uint8))
    mask_resized = label.resize((512, 512), resample=Image.NEAREST) 
    label_tensor = torch.tensor(np.array(mask_resized), dtype=torch.long)
    label_tensor=label_tensor.unsqueeze(0)
    return label_tensor

circle=label_tensor(mask_c)
#rec=label_tensor(mask_rec)
#square=label_tensor(mask_s)
#tri=label_tensor(mask_t)

def dataset(frames, video):
    list_frames=[]
    for f in frames:
        pr_frame=preprocess(f, video.min(), video.max())
        list_frames.append(pr_frame)
    encoded_frames=feature_extractor(list_frames, return_tensors="pt")
    v1 = circle.repeat(len(frames[0]), 1, 1) 
    #v2 = circle.repeat(len(frames[0]), 1, 1)
    #v3 =  rec.repeat(len(frames[0]), 1, 1) 
    #v4 = square.repeat(len(frames[0]), 1, 1)
    #v5 = square.repeat(len(frames[0]), 1, 1)
    #v6 = tri.repeat(len(frames[0]), 1, 1) 
    #all_labels = torch.cat([v1, v2, v3, v4, v5, v6], dim=0)
    encoded_frames["labels"]=v1
    return encoded_frames


class ImageDataset(Dataset):
    def __init__(self, pixel_values, labels):
        self.pixel_values = pixel_values
        self.labels = labels

    def __len__(self):
        return self.pixel_values.shape[0]

    def __getitem__(self, idx):
        return {
            'pixel_values': self.pixel_values[idx],
            'labels': self.labels[idx]
        }
        
validation_dataset=dataset(sampled_frames, video)

#Evaluate

with torch.no_grad():
    outputs = model(validation_dataset["pixel_values"])

logits = outputs.logits  
logits_upsampled = F.interpolate(logits, size=(512, 512), mode='bilinear', align_corners=False)
predicted_mask = logits_upsampled.argmax(dim=1)  #if thresholding is used
#probs = F.softmax(logits_upsampled, dim=1) # if used for softmax, uncomment
#soft_probs = probs[:, 1, :, :] # if used for softmax, uncomment

c_mask=circle.repeat(len(sampled_frames),1,1)


def binary_precision_recall_f1(pred, target, positive_class=1):
    pred = pred.view(-1)
    target = target.view(-1)

    TP = ((pred == positive_class) & (target == positive_class)).sum().item()
    FP = ((pred == positive_class) & (target != positive_class)).sum().item()
    FN = ((pred != positive_class) & (target == positive_class)).sum().item()

    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    return precision, recall, f1

precision, recall, f1 = binary_precision_recall_f1(predicted_mask, c_mask) 

print(f"Precision is {precision}")
print(f"Recall is {recall}")
print(f"F1 score is {f1}")

predicted_mask=predicted_mask.float() #if softmax is used, then put soft_probs instead of prediction_mask here
predicted_mask_4d = F.interpolate(predicted_mask[5].unsqueeze(0).unsqueeze(0), size=(480, 640), mode='bilinear', align_corners=False)
predicted_mask_resized=predicted_mask_4d.squeeze(0).squeeze(0)

predicted_mask_4d_one = F.interpolate(predicted_mask[10].unsqueeze(0).unsqueeze(0), size=(480, 640), mode='bilinear', align_corners=False)
predicted_mask_resized_one=predicted_mask_4d_one.squeeze(0).squeeze(0)

predicted_mask_4d_two = F.interpolate(predicted_mask[20].unsqueeze(0).unsqueeze(0), size=(480, 640), mode='bilinear', align_corners=False)
predicted_mask_resized_two=predicted_mask_4d_two.squeeze(0).squeeze(0)

images = [predicted_mask_resized, predicted_mask_resized_one, predicted_mask_resized_two]

#Visualize

fig, axes = plt.subplots(1, 3, figsize=(12, 4))

for i, ax in enumerate(axes):
    ax.imshow(images[i].cpu(), cmap='gray')
    ax.axis('off')
    ax.set_title(f"Image {i+1}")
plt.tight_layout()
plt.show()







 

